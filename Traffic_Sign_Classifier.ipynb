{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Traffic Sign Recognition Classifier\n",
    "\n",
    "This notebook contains the core functionality, which is required to successfully recognize traffic signs from the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) provided by the Institue for Neuroinformatics of the Ruhr-Universität Bochum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from sklearn.utils import shuffle # for shuffling the training data\n",
    "import cv2 # for grayscaling\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import io # read in images from the web\n",
    "from pandas.io.parsers import read_csv\n",
    "import math\n",
    "import scipy.ndimage\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Data successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the pickled data based on the location of training, validation and testing data\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "training_file = 'data_set/train.p'\n",
    "validation_file = 'data_set/valid.p'\n",
    "testing_file = 'data_set/test.p'\n",
    "\n",
    "# It is good practice to use the \"with\" keyword when dealing with file objects.\n",
    "# The advantage is that the file is properly closed after its suite finishes, even if an exception is raised at some point.\n",
    "\n",
    "with open(training_file, mode='rb') as f: # rb -> r(ead only) and b opens the file in binary mode.\n",
    "    train = pickle.load(f) # train is a dictionary\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "# NumPy arrays\n",
    "X_train, y_train = train['features'], train['labels'] # 'features' and 'labels' are keys from the dictionary train\n",
    "# labels (und somit y_train) enthält die GroundTruth-Daten (= die korrekten labels): [41, 41, 41, ... 25, 25, 25]\n",
    "# mit Länge = Anzahl der Bilder im training set \n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print('Data successfully loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Visualization\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height of the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES TAKE ON THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 34799\n",
      "Length of ground truth labels = 34799\n",
      "Number of validation samples = 4410\n",
      "Number of testing samples = 12630\n",
      "Original Shape of the last image in the training set (width x height):  230 x 201\n",
      "Shape of the first image in the training set: (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "print(\"Number of training samples =\", n_train)\n",
    "\n",
    "# length of ground truth labels\n",
    "n_train_y = len(y_train)\n",
    "print(\"Length of ground truth labels =\", n_train_y)\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = len(X_valid)\n",
    "print(\"Number of validation samples =\", n_validation)\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "print(\"Number of testing samples =\", n_test)\n",
    "\n",
    "# Original Shape of one traffic sign image (here: taking the last image in the training set)\n",
    "image_shape_width = train['sizes'][-1][0]\n",
    "image_shape_height = train['sizes'][-1][1]\n",
    "print(\"Original Shape of the last image in the training set (width x height): \", image_shape_width, \"x\", image_shape_height)\n",
    "\n",
    "# Shape of one traffic sign ((here: taking the first image in the training set))\n",
    "image_shape = X_train[0].shape\n",
    "print(\"Shape of the first image in the training set: {}\".format(image_shape)) # print out the shape of one image to know what the dimensions of the data are\n",
    "\n",
    "# Number of unique classes/labels in the training set\n",
    "n_classes = len(set(train['labels'])) # first make a set out of the dict \"train\" to remove duplicates, then count the elements in that set\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualization of the original dataset\n",
    "\n",
    "Visualizing the German Traffic Signs Dataset using the pickled file(s).\n",
    "\n",
    "It can be interesting to look at the distribution of classes in the training, validation and test set. Is the distribution the same? Are there more examples of some classes than others?\n",
    "\n",
    "Let's take a look on the training dataset. We will check each class (type of traffic sign), counting its number of samples and plotting 10 random images. (Notice: Thank you to navoshta for providing this code. Source: https://github.com/navoshta/traffic-signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "signnames = read_csv(\"signnames.csv\").values[:, 1] # the second column of csv contains the signnames\n",
    "\n",
    "sign_classes, class_indices, class_counts = np.unique(y_train, return_index = True, return_counts = True)\n",
    "\n",
    "col_width = max(len(name) for name in signnames)\n",
    "\n",
    "# display the distribution of classes in the training set\n",
    "plt.bar(np.arange(43), class_counts, align='center')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of training examples')\n",
    "plt.xlim([-1, 43])\n",
    "plt.show()\n",
    "\n",
    "# for each class count its number of samples and plot 10 random images\n",
    "for c, c_index, c_count in zip(sign_classes, class_indices, class_counts):\n",
    "    print(\"Class %i: %-*s  %s samples\" % (c, col_width, signnames[c], str(c_count)))\n",
    "    fig = plt.figure(figsize = (6, 1))\n",
    "    fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "    random_indices = random.sample(range(c_index, c_index + c_count), 10)\n",
    "    for i in range(10):\n",
    "        axis = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "        axis.imshow(X_train[random_indices[i]])\n",
    "    plt.show()\n",
    "    print(\"--------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classes are highly **underrepresented**. Some have only have 200 samples or even less, which is not enough for most of the models to perform well. It is also fairly unbalanced which means some classes are represented to significantly lower extent than others. This could be fixed with **Data augmentation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "Augmenting the training set helps improving the model. It makes the model more robust to slight variations, and hence prevents the model from overfitting.\n",
    "Augmenting techniques are \"cheap tricks\" because no additional data needs to be collected and only a small mount on additional computing resources are needed but performance can significantly be improved.\n",
    "\n",
    "#### Flipping\n",
    "\n",
    "Signs like \"Ahead Only\" are horizontally and/or vertically symmetrical. These can be simply flipped, which would allow us to get twice as much data for these classes.\n",
    "\n",
    "Other signs like \"Turn right ahead\" and \"Turn left ahead\" are some kind of interchageable pairs. These can in a first step be flipped and then be assigned to the corresponding, pairing class. In this case the number of samples could be increased by a factor of around 4.\n",
    "\n",
    "#### Translation, scaling (zoom) and rotation\n",
    "\n",
    "CNNs have built-in invariance to small translations, scaling and rotations. The training doesn't contain such mutations, so we will add those. In this project we implement rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data augmentation technique: Rotation\n",
    "degrees = 10\n",
    "\n",
    "# Rotating images\n",
    "degrees_positive = 10\n",
    "X_train_rotated_positive = []\n",
    "for i in range(len(X_train)):\n",
    "    rotated_image = scipy.ndimage.rotate(X_train[i], degrees_positive)\n",
    "    X_train_rotated_positive.append(rotated_image)\n",
    "    \n",
    "degrees_negative = 350\n",
    "X_train_rotated_negative = []\n",
    "for i in range(len(X_train)):\n",
    "    rotated_image = scipy.ndimage.rotate(X_train[i], degrees_negative)\n",
    "    X_train_rotated_negative.append(rotated_image)    \n",
    "\n",
    "# Crop image, due to other image size after rotation. Attention: it's not automated! It fit's currently to the 10° rotation.\n",
    "for i in range(len(X_train_rotated_positive)):\n",
    "    X_train_rotated_positive[i] = X_train_rotated_positive[i][2:34,2:34]   # box=(y:y+crop, x:x+crop)\n",
    "\n",
    "for i in range(len(X_train_rotated_negative)):\n",
    "    X_train_rotated_negative[i] = X_train_rotated_negative[i][2:34,2:34]   # box=(y:y+crop, x:x+crop)\n",
    "    \n",
    "# appending rotated images to training set\n",
    "\n",
    "# Convert the data into list type to use the method \"append\"\n",
    "X_train = list(X_train)\n",
    "\n",
    "# combine the Lists\n",
    "for i in range(len(X_train_rotated_positive)):\n",
    "    X_train.append(X_train_rotated_positive[i])\n",
    "    X_train.append(X_train_rotated_negative[i])\n",
    "    \n",
    "#Convert the data back to a np.array\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# New number of training examples (after data augmentation)\n",
    "new_n_train = len(X_train)\n",
    "print(\"New number of training samples after data augmentation =\", new_n_train)\n",
    "\n",
    "\n",
    "# do the same for the labels y_train\n",
    "# Convert the data into list type to use the method \"append\"\n",
    "y_train = list(y_train)\n",
    "\n",
    "# lengthen the list\n",
    "for i in range(len(y_train)):\n",
    "    y_train.append(y_train[i])\n",
    "    y_train.append(y_train[i])\n",
    "    \n",
    "#Convert the data back to a np.array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# New length of ground truth labels\n",
    "new_n_train_y = len(y_train)\n",
    "print(\"New length of ground truth labels =\", new_n_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example for rotation\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n",
    "ax1.imshow(X_train[3000])\n",
    "ax1.set_title('Original')    \n",
    "ax1.axis('ON')  # clear x- and y-axes\n",
    "ax2.imshow(X_train_rotated_positive[3000])\n",
    "ax2.set_title('Rotated +10°')    \n",
    "ax2.axis('ON')  # clear x- and y-axes\n",
    "ax3.imshow(X_train_rotated_negative[3000])\n",
    "ax3.set_title('Rotated -10°')   \n",
    "ax3.axis('ON')  # clear x- and y-axes\n",
    "plt.savefig('./examples/Figure_original_rotated.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "#### Normalizing\n",
    "Normalizing helps the network to converge faster. It makes it a lot easier for the optimizer to proceed numerically.\n",
    "It is required to normalize the image data so that the data has mean zero and equal variance. For image data, `(pixel - 128)/ 128` is a quick way to approximately normalize the data. It doesn't change the content of the images.\n",
    "\n",
    "#### Single-channel images (e.g. grayscale)\n",
    "I will only use a single channel in my model, e.g. grayscale images instead of color ones. As Pierre Sermanet and Yann LeCun mentioned in [their paper](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf), using color channels didn't seem to improve things a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = cv2.normalize(X_train[i],None,0,128,cv2.NORM_MINMAX)\n",
    "    \n",
    "for i in range(len(X_valid)):\n",
    "    X_valid[i] = cv2.normalize(X_valid[i],None,0,128,cv2.NORM_MINMAX)\n",
    "    \n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = cv2.normalize(X_test[i],None,0,128,cv2.NORM_MINMAX)\n",
    "\n",
    "# Alternatively\n",
    "#X_train = (X_train - 128)/128\n",
    "#X_valid = (X_valid - 128)/128\n",
    "#X_test = (X_test - 128)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale\n",
    "X_train = np.sum(X_train/3, axis = 3, keepdims=True)\n",
    "X_valid = np.sum(X_valid/3, axis = 3, keepdims=True)\n",
    "X_test = np.sum(X_test/3, axis = 3, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random image to demonstrate how it changes through preprocessing\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "print('image.shape --> ', image.shape)\n",
    "print('image.size --> ', image.size)\n",
    "print('image.dtype --> ', image.dtype)\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design a Model Architecture (Deep Learning model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, mu, sigma, strides, strides_pool, keep_prob):\n",
    "    \n",
    "    # Layer 1: Convolutional.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 1, 16), mean = mu, stddev = sigma)) \n",
    "    conv1_b = tf.Variable(tf.zeros(16))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=strides, padding='VALID') + conv1_b # convolve the filter over the images\n",
    "    print(\"Shape after 1st convolutional layer: \", conv1.shape)\n",
    "\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # Layer 2: Pooling. \n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=strides_pool, padding='VALID')\n",
    "    print(\"Shape after 1st pooling: \", conv1.shape)\n",
    "    \n",
    "    # Layer 3: Convolutional. \n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 16, 64), mean = mu, stddev = sigma)) \n",
    "    conv3_b = tf.Variable(tf.zeros(64))\n",
    "    conv3   = tf.nn.conv2d(conv1, conv3_W, strides=strides, padding='VALID') + conv3_b # convolve the filter over the images\n",
    "    print(\"Shape after 2nd convolutional layer: \", conv3.shape)\n",
    "\n",
    "    # Activation.\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    # Layer 4: Pooling. \n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=strides_pool, padding='VALID')\n",
    "    print(\"Shape after 2nd pooling: \", conv3.shape)\n",
    "\n",
    "    # Layer 5: Convolutional.\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 64, 256), mean = mu, stddev = sigma))\n",
    "    conv5_b = tf.Variable(tf.zeros(256))\n",
    "    conv5   = tf.nn.conv2d(conv3, conv5_W, strides=strides, padding='VALID') + conv5_b\n",
    "    print(\"Shape after 3rd convolutional layer: \", conv5.shape)\n",
    "    \n",
    "    # Activation.\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "\n",
    "    # Layer 6: Pooling. \n",
    "    conv5 = tf.nn.max_pool(conv5, ksize=[1, 2, 2, 1], strides=strides_pool, padding='VALID')\n",
    "    print(\"Shape after 3rd pooling: \", conv5.shape)\n",
    "\n",
    "    # Layer 7: Fully Connected (flatten into a vector).\n",
    "    fc0 = flatten(conv5)\n",
    "    print(\"Shape after flatten: \", fc0.shape)\n",
    "    \n",
    "    # Layer 8: Fully Connected. Output = 120 (= width of fully connected layer).\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(1024, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    print(\"Shape after 1st fully connecting: \", fc1.shape)\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Dropout.\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    print(\"Shape after dropout: \", fc1.shape)\n",
    "\n",
    "    # Output = 43 (because n_classes=43).\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, n_classes), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(n_classes))\n",
    "    logits = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    print(\"Shape after 2nd fully connecting: \", logits.shape)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train and Evaluate the Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 # tells TensorFlow how many times to run our training data through the network\n",
    "# in general the more epochs, the better our model will train but also the longer training will take.\n",
    "\n",
    "BATCH_SIZE = 128 # tells TensorFlow how many training images to run through the network at a time\n",
    "# the larger the batch size, the faster our model will train, but our processor may have a memory limit on how large a batch\n",
    "# it can run\n",
    "\n",
    "rate = 0.0005 # tells TensorFlow how quickly to update the network's weights; 0.001 is a good default value but can be experimented with\n",
    "mu = 0 # used in tf.truncated_normal() --> see the model architecture LeNet() below\n",
    "sigma = 0.1 # dito\n",
    "\n",
    "# strides 's'. These are passed into LeNet()\n",
    "strides = [1, 1, 1, 1] # [batch, y_direction, x_direction, input_channels]\n",
    "strides_pool = [1, 2, 2, 1] # dito\n",
    "\n",
    "# Dropout technique\n",
    "keep_prob = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels (groundtruth data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1)) # initialize batch size to \"None\" which allows the placeholder to later accept a batch of any size\n",
    "y = tf.placeholder(tf.int32, (None)) # not yet one-hot encoded\n",
    "\n",
    "one_hot_y = tf.one_hot(y, 43) # 1-hot encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline\n",
    "`training_operation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after 1st convolutional layer:  (?, 30, 30, 16)\n",
      "Shape after 1st pooling:  (?, 15, 15, 16)\n",
      "Shape after 2nd convolutional layer:  (?, 13, 13, 64)\n",
      "Shape after 2nd pooling:  (?, 6, 6, 64)\n",
      "Shape after 3rd convolutional layer:  (?, 4, 4, 256)\n",
      "Shape after 3rd pooling:  (?, 2, 2, 256)\n",
      "Shape after flatten:  (?, 1024)\n",
      "Shape after 1st fully connecting:  (?, 120)\n",
      "Shape after dropout:  (?, 120)\n",
      "Shape after 2nd fully connecting:  (?, 43)\n"
     ]
    }
   ],
   "source": [
    "# pass the input data x (batch of images) to the LeNet function (deep neural network) to calculate logits\n",
    "logits = LeNet(x, mu, sigma, strides, strides_pool, keep_prob)\n",
    "\n",
    "# cross entropy is a measure of how different the logits are from the ground truth training labels\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy) # averages the CE from all of the training images\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate) # uses Adam algorithm to minimize the loss function similarly to SGD.\n",
    "                                                         # little bit more sophisticated than SGD\n",
    "training_operation = optimizer.minimize(loss_operation) # run the minimize function on the optimzier which uses backprop to\n",
    "# upgrade the network and minimize our training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation pipeline\n",
    "`accuracy_operation`\n",
    "\n",
    "Evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the logit prediction to the 1-hot encoded ground truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "\n",
    "# Calculate the model's overall accuracy by averaging the individual prediction accuracies\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver() # create an instance/object of tf.train.Saver() class\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE] # batch the dataset\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y}) # run the batched dataset through the evaluation pipeline\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate the model\n",
    "\n",
    "Run the training data through the training pipeline to train the model -->`training_operation`\n",
    "\n",
    "Before each epoch, training set will be shuffled to ensure that training is not biased by the order of the images.\n",
    "\n",
    "After each epoch, the validation accuracy and accuracy on the training set are measured. A low accuracy on the training and validation sets implies underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting.\n",
    "\n",
    "The model will be saved after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE): # range(start, stop, step)\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x = X_train[offset:end] # break training data into batches\n",
    "            batch_y = y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y}) # train the model on each batch\n",
    "            \n",
    "        # at the end of each epoch, we evaluate the model on our validation data\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        \n",
    "        # also evaluate the model on training data to see if the model is over- or underfitted\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        \n",
    "        # also evaluate the model on test data (different from validation data)\n",
    "        test_accuracy = evaluate(X_test, y_test)\n",
    "        \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print(\"Accuracy on the training set = {:.3f}\".format(training_accuracy))\n",
    "        print(\"Accuracy on the test set = {:.3f}\".format(test_accuracy))\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    saver.save(sess, 'lenet') # save the model to be able to load it up later and modify it or evaluate it on test dataset\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open/Restore the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lenet\n",
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "# from: https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('lenet.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Use the model to make predictions on new images found on the web\n",
    "\n",
    "To give more insight into how the model is performing, five pictures of German traffic signs from the web are taken and applied to the model to predict the traffic sign type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Load images from .png files to `X_custom` NumPy array\n",
    "X_custom = np.empty([0, 32, 32, 3], dtype = np.float64) # numpy.empty(shape, dtype); shape > int or tuple of int; \n",
    "                                                        # 0 gibt damit die Länge an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the images into a file list (no numpy array! only a list)\n",
    "additional_file_list = [f for f in glob.glob(\"examples/web/\"+'*.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in image has shape:  (32, 32, 4)\n",
      "read in image has shape:  (32, 32, 4)\n",
      "read in image has shape:  (32, 32, 4)\n",
      "read in image has shape:  (32, 32, 4)\n",
      "read in image has shape:  (32, 32, 4)\n",
      "len(X_custom):  5\n",
      "X_custom.shape:  (5, 32, 32, 3)\n",
      "X_custom.dtype:  float64\n"
     ]
    }
   ],
   "source": [
    "# Bilder einlesesn und X_custom aufbauen\n",
    "\n",
    "for i in range(len(additional_file_list)):\n",
    "    image = io.imread(os.getcwd() + '/examples/web/' + \"example_{0:0>5}\".format(i + 1) + '.png')\n",
    "    image_resized = cv2.resize(image, (32, 32))\n",
    "    print(\"read in image has shape: \", image_resized.shape)\n",
    "    X_custom = np.append(X_custom, [image_resized[:, :, :3]], axis = 0) # X_custom wird kopiert und [image[:, :, :3]] drangehängt\n",
    "        \n",
    "print(\"len(X_custom): \", len(X_custom))\n",
    "print(\"X_custom.shape: \", X_custom.shape)\n",
    "print(\"X_custom.dtype: \", X_custom.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide labels for the images found on the web:\n",
    "\n",
    "\"\"\"\n",
    "# eigene Bilder\n",
    "y_custom = np.array([\n",
    "    25, # \"example_00001\"\n",
    "    23, # \"example_00002\"\n",
    "    16, # \"example_00003\"\n",
    "    17, # \"example_00004\"\n",
    "    39, # \"example_00005\"\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# DiegeID\n",
    "y_custom = np.array([\n",
    "    14, # \"example_00001\"\n",
    "    13, # \"example_00002\"\n",
    "    12, # \"example_00003\"\n",
    "    7, # \"example_00004\"\n",
    "    17, # \"example_00005\"\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# wikipedia\n",
    "y_custom = np.array([\n",
    "    13, # \"example_00001\"\n",
    "    3, # \"example_00002\"\n",
    "    12, # \"example_00003\"\n",
    "    17, # \"example_00004\"\n",
    "    35, # \"example_00005\"\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "# train images\n",
    "y_custom = np.array([\n",
    "    14, # \"example_00001\"\n",
    "    13, # \"example_00002\"\n",
    "    12, # \"example_00003\"\n",
    "    7, # \"example_00004\"\n",
    "    17, # \"example_00005\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57275c93c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "for i in range(len(X_custom)):\n",
    "    axis = fig.add_subplot(1, len(additional_file_list), i + 1, xticks=[], yticks=[])\n",
    "    web_image = io.imread(os.getcwd() + '/examples/web/' + \"example_{0:0>5}\".format(i + 1) + '.png')\n",
    "    axis.imshow(web_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ: Plotten mit fester Anzahl von Bildern, z.B. 5\n",
    "\n",
    "'''\n",
    "f, (ax1, ax2, ax3,ax4,ax5) = plt.subplots(1, 5, figsize=(20, 32))\n",
    "ax1.imshow(additional_imag_list[0])\n",
    "ax1.axis('ON')\n",
    "ax2.imshow(additional_imag_list[1])\n",
    "ax2.axis('ON')\n",
    "ax3.imshow(additional_imag_list[2])\n",
    "ax3.axis('ON')\n",
    "ax4.imshow(additional_imag_list[3])\n",
    "ax4.axis('ON')\n",
    "ax5.imshow(additional_imag_list[4])\n",
    "ax5.axis('ON')\n",
    "#plt.savefig('./examples/name_of_file.jpg', dpi=300)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image\n",
    "Output the prediction for each image using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the images with the same techniques used earlier (normalize and grayscale)\n",
    "\n",
    "# Normalize\n",
    "for i in range(len(X_custom)):\n",
    "    X_custom[i] = cv2.normalize(X_custom[i],None,0,128,cv2.NORM_MINMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_custom[0].shape:  (32, 32, 1)\n",
      "X_custom.shape:  (5, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Grayscale\n",
    "X_custom = np.sum(X_custom/3, axis = 3, keepdims=True)\n",
    "print(\"X_custom[0].shape: \", X_custom[0].shape)\n",
    "print(\"X_custom.shape: \", X_custom.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try this to make a prediction for a single image\n",
    "\n",
    "\"\"\"\n",
    "x_1 = tf.cast(X_custom[0], tf.float32) # to resolve \"TypeError: Value passed to parameter 'input' has DataType float64 not in list of allowed values: float16, float32\"\n",
    "print(\"x_1: \",x_1)\n",
    "x_1_expanded = tf.expand_dims(x_1, 0)\n",
    "y_1 = y_custom[0]\n",
    "logits = LeNet(x_1_expanded, mu, sigma, strides, strides_pool, keep_prob)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance\n",
    "Calculate the accuracy for these 5 new images. \n",
    "For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on images found on the web = 0.200\n"
     ]
    }
   ],
   "source": [
    "## short version with calling evaluate(X_custom, y_custom) directly. Only can run when long version (next cell) works!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_custom)\n",
    "    web_accuracy = evaluate(X_custom, y_custom)\n",
    "    print(\"Accuracy on images found on the web = {:.3f}\".format(web_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## long version (step by step) with batches\n",
    "\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_custom) # 5\n",
    "    total_accuracy = 0\n",
    "    BATCH_SIZE = 1\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_custom[offset:offset+BATCH_SIZE]\n",
    "        batch_y = y_custom[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y}) # run the batched dataset through the evaluation pipeline\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        \n",
    "print(total_accuracy / num_examples)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## long version (step by step) without batches\n",
    "\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_custom) # 5\n",
    "    total_accuracy = 0\n",
    "    for i in range(num_examples):\n",
    "        x_i = X_custom[i:i+1]\n",
    "        y_i = y_custom[i:i+1]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: x_i, y: y_i})\n",
    "        total_accuracy += (accuracy * len(x_i))\n",
    "        \n",
    "print(total_accuracy / num_examples)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the new images, print out the model's softmax probabilities to show the **certainty** of the model's predictions (limit the output to the top 5 probabilities for each image). [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. \n",
    "\n",
    "The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image.\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. `tf.nn.top_k` is used to choose the three classes with the highest probability:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKV2(values=array([[  9.99983907e-01,   9.61791920e-06,   6.18651757e-06,\n",
      "          2.10961829e-07,   1.41493339e-09],\n",
      "       [  1.00000000e+00,   4.30695435e-09,   4.03354572e-11,\n",
      "          2.94106905e-12,   1.71736009e-12],\n",
      "       [  9.99996185e-01,   3.33412754e-06,   3.27883072e-07,\n",
      "          1.35806530e-07,   4.67445904e-09],\n",
      "       [  9.87150908e-01,   1.28438100e-02,   5.30609350e-06,\n",
      "          2.01369788e-09,   1.02841780e-09],\n",
      "       [  4.68094319e-01,   4.03428644e-01,   9.08890367e-02,\n",
      "          3.75672467e-02,   1.99605383e-05]], dtype=float32), indices=array([[12, 22, 42, 21, 11],\n",
      "       [36, 17, 40, 12, 22],\n",
      "       [ 0, 12, 36, 22, 17],\n",
      "       [ 0, 36, 22, 26, 17],\n",
      "       [17, 11, 22, 36, 12]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n",
    "\n",
    "top_5_probabilities = tf.nn.top_k(tf.nn.softmax(logits), k=5) # tf.nn.top_k(input, k=?)\n",
    "\n",
    "keep_prob = 1.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    top_5_probabilities_output = sess.run(top_5_probabilities, feed_dict = {x:X_custom, y:y_custom})\n",
    "    print(top_5_probabilities_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 (Optional): Visualize the Neural Network's State with Test Images\n",
    "\n",
    " This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n",
    "\n",
    " Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the [LeNet lab's](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable.\n",
    "\n",
    "For an example of what feature map outputs look like, check out NVIDIA's results in their paper [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
